{"cells":[{"cell_type":"markdown","source":["## Model Deployment with Spark Serving \nIn this example, we try to movie recommendations from the *Movie Ratings* dataset. Then we will use Spark serving to deploy it as a realtime web service. \nFirst, we import needed packages:"],"metadata":{}},{"cell_type":"code","source":["import sys\nimport numpy as np\nimport pandas as pd\nimport mmlspark\nimport os\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{"collapsed":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Now let's read the data and split it to train and test sets:"],"metadata":{}},{"cell_type":"code","source":["#Columns\nuserCol='UserId'\nitemCol='MovieId'\nratingCol='Rating'\n\nuserColIndex = userCol.replace(\"Id\",\"Index\")\nitemColIndex = itemCol.replace(\"Id\",\"Index\")\n\n\n# Download Movie Lens\nbasedataurl = \"http://aka.ms\" \ndatafile = \"MovieRatings.csv\"\n\ndatafile_dbfs = os.path.join(\"/dbfs\", datafile)\n\nif os.path.isfile(datafile_dbfs):\n    print(\"found {} at {}\".format(datafile, datafile_dbfs))\nelse:\n    print(\"downloading {} to {}\".format(datafile, datafile_dbfs))\n    urllib.request.urlretrieve(os.path.join(basedataurl, datafile), datafile_dbfs)\n    \ndata_all = sqlContext.read.format('csv')\\\n                     .options(header='true', delimiter=',', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true')\\\n                     .load(datafile)    \ndata_all.printSchema()\ndisplay(data_all)\n\ntrain, test = data_all.randomSplit([0.75, 0.25], seed=123)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Next, we will create a Cross Validator pipeline, in order to tune a Spark ALS model."],"metadata":{}},{"cell_type":"code","source":["indexerContacts = StringIndexer(inputCol=userCol, outputCol=userColIndex, handleInvalid='keep').fit(data_all)\nindexerRules = StringIndexer(inputCol=itemCol, outputCol=itemColIndex, handleInvalid='keep').fit(data_all)\n\nals = ALS(maxIter=5, userCol=userColIndex, itemCol=itemColIndex, ratingCol=ratingCol, coldStartStrategy=\"drop\")\n\n# put together the pipeline\npipe = Pipeline(stages=[indexerContacts, indexerRules, als])\n\n# Regularization Rates\nregs = [1, 0.1, 0.001]\nparamGrid = ParamGridBuilder().addGrid(als.regParam, regs).build()\n\n\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=ratingCol, predictionCol=\"prediction\")\ncv = CrossValidator(estimator=pipe, evaluator=evaluator, estimatorParamMaps=paramGrid)\ntrain.cache()\nmodel = cv.fit(train)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["testInput = indexerContacts.transform(test).select(\"UserIndex\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now, we will define the webservice input/output.\nFor more information, you can visit the [documentation for Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, from_json, broadcast\nfrom pyspark.sql.types import *\nimport uuid\nfrom mmlspark import request_to_string, string_to_response\n\nserving_inputs = spark.readStream.server() \\\n    .address(\"localhost\", 8898, \"my_api\") \\\n    .load()\\\n    .parseRequest(testInput.schema)\n\nrecommendations = model.bestModel.stages[2].recommendForAllUsers(10).cache()\n\nserving_outputs = serving_inputs \\\n  .join(broadcast(recommendations), 'UserIndex') \\\n  .makeReply(\"recommendations\")\n\nserver = serving_outputs.writeStream \\\n    .server() \\\n    .replyTo(\"my_api\") \\\n    .queryName(\"my_query\") \\\n    .option(\"checkpointLocation\", \"checkpoints-{}\".format(uuid.uuid1())) \\\n    .start()\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Test the webservice"],"metadata":{}},{"cell_type":"code","source":["import requests\ndata = u'{\"UserIndex\":13621.0}'\nr = requests.post(data=data, url=\"http://localhost:8898/my_api\")\nprint(\"Response {}\".format(r.text))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["import requests\ndata = u'{\"UserIndex\":4247.0}'\nr = requests.post(data=data, url=\"http://localhost:8898/my_api\")\nprint(\"Response {}\".format(r.text))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["import time\ntime.sleep(20) # wait for server to finish setting up (just to be safe)\nserver.stop()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":14}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"name":"107 - Model Deployment with Spark Serving","notebookId":3368160449166478,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
