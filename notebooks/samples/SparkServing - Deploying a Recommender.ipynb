{  
   "cells":[  
      {  
         "cell_type":"markdown",
         "source":[  
            "## Model Deployment with Spark Serving \nIn this example, we try to movie recommendations from the *Movie Ratings* dataset. Then we will use Spark serving to deploy it as a realtime web service. \nFirst, we import needed packages:"
         ],
         "metadata":{  

         }
      },
      {  
         "cell_type":"code",
         "source":[  
            "import sys\nimport numpy as np\nimport pandas as pd\nimport mmlspark\nimport os\n\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator"
         ],
         "metadata":{  
            "collapsed":true
         },
         "outputs":[  

         ],
         "execution_count":2
      },
      {  
         "cell_type":"markdown",
         "source":[  
            "Now let's read the data and split it to train and test sets:"
         ],
         "metadata":{  

         }
      },
      {  
         "cell_type":"code",
         "source":[  
            "#Columns\nuserCol='UserId'\nitemCol='MovieId'\nratingCol='Rating'\n\n# Download Movie Lens\nbasedataurl = \"http://aka.ms\" \ndatafile = \"MovieRatings.csv\"\n\ndatafile_dbfs = os.path.join(\"/dbfs\", datafile)\n\nif os.path.isfile(datafile_dbfs):\n    print(\"found {} at {}\".format(datafile, datafile_dbfs))\nelse:\n    print(\"downloading {} to {}\".format(datafile, datafile_dbfs))\n    urllib.request.urlretrieve(os.path.join(basedataurl, datafile), datafile_dbfs)\n    \ndata_all = sqlContext.read.format('csv')\\\n                     .options(header='true', delimiter=',', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true')\\\n                     .load(datafile).cache()    \ndata_all.printSchema()\ndisplay(data_all)"
         ],
         "metadata":{  

         },
         "outputs":[  

         ],
         "execution_count":4
      },
      {  
         "cell_type":"markdown",
         "source":[  
            "Next, we will create a Cross Validator pipeline, in order to tune a Spark ALS model."
         ],
         "metadata":{  

         }
      },
      {  
         "cell_type":"code",
         "source":[  
            "from mmlspark.TrainValidRecommendSplit import TrainValidRecommendSplit\nfrom mmlspark.RecommendationEvaluator import RecommendationEvaluator\n\nals = ALS() \\\n    .setUserCol(userCol) \\\n    .setRatingCol(ratingCol) \\\n    .setItemCol(itemCol) \\\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(als.maxIter, [2, 4, 8]) \\\n    .addGrid(als.regParam, [1, 0.1, 0.01]) \\\n    .build()\n\nitemCount = data_all.select(itemCol).distinct().count()\nevaluator = RecommendationEvaluator().setSaveAll(True).setNumberItems(itemCount)\n\ntvRecommendationSplit = TrainValidRecommendSplit() \\\n    .setEstimator(als) \\\n    .setEvaluator(evaluator) \\\n    .setEstimatorParamMaps(paramGrid) \\\n    .setTrainRatio(0.8) \\\n    .setUserCol(als.getUserCol()) \\\n    .setRatingCol(als.getRatingCol()) \\\n    .setItemCol(als.getItemCol())\n\nmodel = tvRecommendationSplit.fit(data_all)\n\nrecommendations = model.bestModel.recommendForAllUsers(10).cache()"
         ],
         "metadata":{  

         },
         "outputs":[  

         ],
         "execution_count":6
      },
      {  
         "cell_type":"markdown",
         "source":[  
            "Now, we will define the webservice input/output.\nFor more information, you can visit the [documentation for Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md)"
         ],
         "metadata":{  

         }
      },
      {  
         "cell_type":"code",
         "source":[  
            "from pyspark.sql.functions import col, from_json, broadcast\nfrom pyspark.sql.types import *\nimport uuid\nfrom mmlspark import request_to_string, string_to_response\n\nserving_inputs = spark.readStream.server() \\\n    .address(\"localhost\", 8898, \"my_api\") \\\n    .load()\\\n    .parseRequest(data_all.select(userCol).schema)\n\nserving_outputs = serving_inputs \\\n  .join(broadcast(recommendations), userCol) \\\n  .makeReply(\"recommendations\")\n\nserver = serving_outputs.writeStream \\\n    .server() \\\n    .replyTo(\"my_api\") \\\n    .queryName(\"my_query\") \\\n    .option(\"checkpointLocation\", \"checkpoints-{}\".format(uuid.uuid1())) \\\n    .start()\n"
         ],
         "metadata":{  

         },
         "outputs":[  

         ],
         "execution_count":8
      },
      {  
         "cell_type":"markdown",
         "source":[  
            "Test the webservice"
         ],
         "metadata":{  

         }
      },
      {  
         "cell_type":"code",
         "source":[  
            "import requests\ndata = u'{\"UserId\":1}'\nr = requests.post(data=data, url=\"http://localhost:8898/my_api\")\nprint(\"Response {}\".format(r.text))"
         ],
         "metadata":{  

         },
         "outputs":[  

         ],
         "execution_count":10
      },
      {  
         "cell_type":"code",
         "source":[  
            "import requests\ndata = u'{\"UserId\":200}'\nr = requests.post(data=data, url=\"http://localhost:8898/my_api\")\nprint(\"Response {}\".format(r.text))"
         ],
         "metadata":{  

         },
         "outputs":[  

         ],
         "execution_count":11
      },
      {  
         "cell_type":"code",
         "source":[  
            "import time\ntime.sleep(20) # wait for server to finish setting up (just to be safe)\nserver.stop()"
         ],
         "metadata":{  
            "collapsed":true
         },
         "outputs":[  

         ],
         "execution_count":12
      }
   ],
   "metadata":{  
      "anaconda-cloud":{  

      },
      "kernelspec":{  
         "display_name":"Python 3",
         "language":"python",
         "name":"python3"
      },
      "language_info":{  
         "codemirror_mode":{  
            "name":"ipython",
            "version":3
         },
         "file_extension":".py",
         "mimetype":"text/x-python",
         "name":"python",
         "nbconvert_exporter":"python",
         "pygments_lexer":"ipython3",
         "version":"3.6.3"
      },
      "name":"107 - Model Deployment with Spark Serving"
   },
   "nbformat":4,
   "nbformat_minor":0
}
